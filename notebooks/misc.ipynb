{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task type: Supervised Classification\n",
      "Dataset name: credit-g\n",
      "Description: **Author**: Dr. Hans Hofmann  \n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) - 1994    \n",
      "**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html)\n",
      "\n",
      "**German Credit dataset**  \n",
      "This dataset classifies people described by a set of attributes as good or bad credit risks.\n",
      "\n",
      "This dataset comes with a cost matrix: \n",
      "``` \n",
      "Good  Bad (predicted)  \n",
      "Good   0    1   (actual)  \n",
      "Bad    5    0  \n",
      "```\n",
      "\n",
      "It is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).  \n",
      "\n",
      "### Attribute description  \n",
      "\n",
      "1. Status of existing checking account, in Deutsche Mark.  \n",
      "2. Duration in months  \n",
      "3. Credit history (credits taken, paid back duly, delays, critical accounts)  \n",
      "4. Purpose of the credit (car, television,...)  \n",
      "5. Credit amount  \n",
      "6. Status of savings account/bonds, in Deutsche Mark.  \n",
      "7. Present employment, in number of years.  \n",
      "8. Installment rate in percentage of disposable income  \n",
      "9. Personal status (married, single,...) and sex  \n",
      "10. Other debtors / guarantors  \n",
      "11. Present residence since X years  \n",
      "12. Property (e.g. real estate)  \n",
      "13. Age in years  \n",
      "14. Other installment plans (banks, stores)  \n",
      "15. Housing (rent, own,...)  \n",
      "16. Number of existing credits at this bank  \n",
      "17. Job  \n",
      "18. Number of people being liable to provide maintenance for  \n",
      "19. Telephone (yes,no)  \n",
      "20. Foreign worker (yes,no)\n",
      "Number of instances: 1000.0\n",
      "Number of features: 21.0\n",
      "Target feature: class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/acb22av/.conda/envs/automl-early-stop/lib/python3.10/site-packages/openml/tasks/functions.py:442: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  dataset = get_dataset(task.dataset_id, *dataset_args, **get_dataset_kwargs)\n",
      "/users/acb22av/.conda/envs/automl-early-stop/lib/python3.10/site-packages/openml/tasks/task.py:150: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  return datasets.get_dataset(self.dataset_id)\n"
     ]
    }
   ],
   "source": [
    "task = openml.tasks.get_task(31, download_splits=False)\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "print(f\"Task type: {task.task_type}\")\n",
    "print(f\"Dataset name: {dataset.name}\")\n",
    "print(f\"Description: {dataset.description}\")\n",
    "print(f\"Number of instances: {dataset.qualities['NumberOfInstances']}\")\n",
    "print(f\"Number of features: {dataset.qualities['NumberOfFeatures']}\")\n",
    "print(f\"Target feature: {dataset.default_target_attribute}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_child_index(parent_dir, child_name):\n",
    "    try:\n",
    "        directory_contents = os.listdir(parent_dir)\n",
    "        if child_name in directory_contents:\n",
    "            return directory_contents.index(child_name)\n",
    "        return -1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Parent directory {parent_dir} not found\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory results-category3 not found\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "parent_dir = \"results-category3\"\n",
    "child_name = \"cv_early_stop_strategy=current_average_worse_than_mean_best-fold=0-metric=roc_auc_ovr-n_splits=10-optimizer=random_search-pipeline=mlp_classifier-task=146818\"\n",
    "\n",
    "print(find_child_index(parent_dir, child_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick commands\n",
    "1. One exp\n",
    "    ```bash\n",
    "    python e1.py submit --expname \"category3-nsplits-10\" --job-array-limit 1000 --mail-type ALL --mail-user avakhutinskiy1@sheffield.ac.uk\n",
    "    ```\n",
    "\n",
    "    Output:\n",
    "    ```bash\n",
    "    pending: 1080\n",
    "    Submitted batch job 5860009\n",
    "\n",
    "    Due to the job array of the experiment \"category3-nsplits-10\" (1080) exceeding job array limit (1000):\n",
    "    Submitted chunk 0-999 of 1079\n",
    "\n",
    "    To submit the next chunk, run the following command when 80 jobs terminate (succed or fail):\n",
    "\n",
    "        python e1.py submit --expname category3-nsplits-10 --job-array-limit 1000 --chunk-start-idx 1000\n",
    "    ```\n",
    "2. Count files\n",
    "    ```bash\n",
    "    find results-category3/ -name \".flag.submitted\" | wc -l\n",
    "    ```\n",
    "\n",
    "    Output: `1000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis df_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.column_stats import generate_column_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"mlp-nsplits-10\"\n",
    "df_paper  = pd.read_parquet(f\"data-paper/{exp}.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "stats_dict = generate_column_stats(df=df_paper)\n",
    "print(type(stats_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the paper\n",
    "\n",
    "| Aggressive Average Speedup % | Aggressive Datasets Failed | Forgiving Average Speedup % | Forgiving Datasets Failed |\n",
    "| ----------------------------- | ------------------------- | ---------------------------- | ------------------------ |\n",
    "| 301% ± 187%                  | 20/36                     | 174% ± 64%                  | 0/36                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5903/2580808116.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_with_trace = groups.apply(add_incumbent_trace).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute the Incumbent Trace per Dataset/Fold\n",
    "df_paper[\"created_at\"] = pd.to_datetime(df_paper[\"created_at\"])\n",
    "df_sorted = df_paper.sort_values(\"created_at\")\n",
    "\n",
    "groups = df_sorted.groupby([\"setting:task\", \"setting:fold\"])\n",
    "\n",
    "def add_incumbent_trace(grp):\n",
    "    grp = grp.copy()\n",
    "    grp[\"incumbent_val\"] = grp[\"summary:val_mean_roc_auc_ovr\"].cummax()\n",
    "    return grp\n",
    "\n",
    "df_with_trace = groups.apply(add_incumbent_trace).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Aggregate Across Folds per Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis df vs df_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_paper  = pd.read_parquet(\"../data-paper/mlp-nsplits-10.parquet.gzip\")\n",
    "df = pd.read_parquet(\"data/mlp-nsplits-10.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_paper shape: (2732865, 56)\n",
      "df shape:    (3213287, 57)\n",
      "\n",
      "{'setting:seeded_inner_cv'}\n"
     ]
    }
   ],
   "source": [
    "print(f'df_paper shape: {df_paper.shape}')\n",
    "print(f'df shape:    {df.shape}')\n",
    "\n",
    "columns_diff = set(df.columns).difference(df_paper.columns)\n",
    "print(f'\\n{columns_diff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=columns_diff, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_paper columns: Index(['created_at', 'reported_at', 'status',\n",
      "       'metric:roc_auc_ovr [0.0, 1.0] (maximize)',\n",
      "       'summary:val_mean_roc_auc_ovr', 'summary:val_std_roc_auc_ovr',\n",
      "       'summary:test_mean_roc_auc_ovr', 'summary:test_std_roc_auc_ovr',\n",
      "       'summary:test_bagged_roc_auc_ovr', 'summary:split_0:val_roc_auc_ovr',\n",
      "       'summary:split_1:val_roc_auc_ovr', 'summary:split_2:val_roc_auc_ovr',\n",
      "       'summary:split_3:val_roc_auc_ovr', 'summary:split_4:val_roc_auc_ovr',\n",
      "       'summary:split_5:val_roc_auc_ovr', 'summary:split_6:val_roc_auc_ovr',\n",
      "       'summary:split_7:val_roc_auc_ovr', 'summary:split_8:val_roc_auc_ovr',\n",
      "       'summary:split_9:val_roc_auc_ovr', 'summary:split_0:test_roc_auc_ovr',\n",
      "       'summary:split_1:test_roc_auc_ovr', 'summary:split_2:test_roc_auc_ovr',\n",
      "       'summary:split_3:test_roc_auc_ovr', 'summary:split_4:test_roc_auc_ovr',\n",
      "       'summary:split_5:test_roc_auc_ovr', 'summary:split_6:test_roc_auc_ovr',\n",
      "       'summary:split_7:test_roc_auc_ovr', 'summary:split_8:test_roc_auc_ovr',\n",
      "       'summary:split_9:test_roc_auc_ovr',\n",
      "       'config:mlp_classifier:MLPClassifier:activation',\n",
      "       'config:mlp_classifier:MLPClassifier:alpha',\n",
      "       'config:mlp_classifier:MLPClassifier:early_stopping',\n",
      "       'config:mlp_classifier:MLPClassifier:hidden_layer_depth',\n",
      "       'config:mlp_classifier:MLPClassifier:learning_rate',\n",
      "       'config:mlp_classifier:MLPClassifier:learning_rate_init',\n",
      "       'config:mlp_classifier:MLPClassifier:momentum',\n",
      "       'config:mlp_classifier:MLPClassifier:num_nodes_per_layer',\n",
      "       'config:mlp_classifier:encoding:categorical:OrdinalEncoder:min_frequency',\n",
      "       'config:mlp_classifier:encoding:categorical:one_hot:__choice__',\n",
      "       'config:mlp_classifier:encoding:numerical:SimpleImputer:strategy',\n",
      "       'config:mlp_classifier:encoding:categorical:one_hot:OneHotEncoder:max_categories',\n",
      "       'setting:root', 'setting:experiment_seed', 'setting:task',\n",
      "       'setting:fold', 'setting:metric', 'setting:pipeline',\n",
      "       'setting:optimizer', 'setting:n_splits',\n",
      "       'setting:cv_early_stop_strategy', 'setting:n_cpus', 'setting:memory_gb',\n",
      "       'setting:time_seconds', 'setting:minimum_trials', 'setting:wait',\n",
      "       'setting:openml_cache_directory'],\n",
      "      dtype='object')\n",
      "df columns: Index(['created_at', 'reported_at', 'status',\n",
      "       'metric:roc_auc_ovr [0.0, 1.0] (maximize)',\n",
      "       'summary:val_mean_roc_auc_ovr', 'summary:val_std_roc_auc_ovr',\n",
      "       'summary:test_mean_roc_auc_ovr', 'summary:test_std_roc_auc_ovr',\n",
      "       'summary:test_bagged_roc_auc_ovr', 'summary:split_0:val_roc_auc_ovr',\n",
      "       'summary:split_1:val_roc_auc_ovr', 'summary:split_2:val_roc_auc_ovr',\n",
      "       'summary:split_3:val_roc_auc_ovr', 'summary:split_4:val_roc_auc_ovr',\n",
      "       'summary:split_5:val_roc_auc_ovr', 'summary:split_6:val_roc_auc_ovr',\n",
      "       'summary:split_7:val_roc_auc_ovr', 'summary:split_8:val_roc_auc_ovr',\n",
      "       'summary:split_9:val_roc_auc_ovr', 'summary:split_0:test_roc_auc_ovr',\n",
      "       'summary:split_1:test_roc_auc_ovr', 'summary:split_2:test_roc_auc_ovr',\n",
      "       'summary:split_3:test_roc_auc_ovr', 'summary:split_4:test_roc_auc_ovr',\n",
      "       'summary:split_5:test_roc_auc_ovr', 'summary:split_6:test_roc_auc_ovr',\n",
      "       'summary:split_7:test_roc_auc_ovr', 'summary:split_8:test_roc_auc_ovr',\n",
      "       'summary:split_9:test_roc_auc_ovr',\n",
      "       'config:mlp_classifier:MLPClassifier:activation',\n",
      "       'config:mlp_classifier:MLPClassifier:alpha',\n",
      "       'config:mlp_classifier:MLPClassifier:early_stopping',\n",
      "       'config:mlp_classifier:MLPClassifier:hidden_layer_depth',\n",
      "       'config:mlp_classifier:MLPClassifier:learning_rate',\n",
      "       'config:mlp_classifier:MLPClassifier:learning_rate_init',\n",
      "       'config:mlp_classifier:MLPClassifier:momentum',\n",
      "       'config:mlp_classifier:MLPClassifier:num_nodes_per_layer',\n",
      "       'config:mlp_classifier:encoding:categorical:OrdinalEncoder:min_frequency',\n",
      "       'config:mlp_classifier:encoding:categorical:one_hot:__choice__',\n",
      "       'config:mlp_classifier:encoding:numerical:SimpleImputer:strategy',\n",
      "       'config:mlp_classifier:encoding:categorical:one_hot:OneHotEncoder:max_categories',\n",
      "       'setting:root', 'setting:experiment_seed', 'setting:task',\n",
      "       'setting:fold', 'setting:metric', 'setting:pipeline',\n",
      "       'setting:optimizer', 'setting:n_splits',\n",
      "       'setting:cv_early_stop_strategy', 'setting:n_cpus', 'setting:memory_gb',\n",
      "       'setting:time_seconds', 'setting:minimum_trials', 'setting:wait',\n",
      "       'setting:openml_cache_directory'],\n",
      "      dtype='object')\n",
      "df_paper index: Index(['trial-2', 'trial-0', 'trial-3', 'trial-6', 'trial-5', 'trial-4',\n",
      "       'trial-9', 'trial-10', 'trial-7', 'trial-11',\n",
      "       ...\n",
      "       'trial-1435', 'trial-1439', 'trial-1437', 'trial-1430', 'trial-1438',\n",
      "       'trial-1442', 'trial-1440', 'trial-1444', 'trial-1445', 'trial-1443'],\n",
      "      dtype='object', name='name', length=2732865)\n",
      "df index: Index(['trial-2', 'trial-0', 'trial-3', 'trial-6', 'trial-5', 'trial-4',\n",
      "       'trial-9', 'trial-7', 'trial-10', 'trial-11',\n",
      "       ...\n",
      "       'trial-1813', 'trial-1815', 'trial-1817', 'trial-1816', 'trial-1820',\n",
      "       'trial-1821', 'trial-1819', 'trial-1822', 'trial-1824', 'trial-1823'],\n",
      "      dtype='object', name='name', length=3213287)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_paper columns:\", df_paper.columns)\n",
    "print(\"df columns:\", df.columns)\n",
    "print(\"df_paper index:\", df_paper.index)\n",
    "print(\"df index:\", df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extra trials in df: 7667\n",
      "Some extra trials in df: ['trial-34242', 'trial-34959', 'trial-35022', 'trial-36874', 'trial-33443', 'trial-30433', 'trial-35092', 'trial-34349', 'trial-33891', 'trial-35648']\n",
      "Number of trials in df_paper not in df: 0\n",
      "Number of duplicate trial-ids in df: 3175865\n",
      "Number of duplicate trial-ids in df_paper: 2703110\n"
     ]
    }
   ],
   "source": [
    "idx_df_paper = set(df_paper.index)\n",
    "idx_df = set(df.index)\n",
    "\n",
    "extra_trials = idx_df - idx_df_paper\n",
    "print(f\"Number of extra trials in df: {len(extra_trials)}\")\n",
    "print(\"Some extra trials in df:\", list(extra_trials)[:10])\n",
    "\n",
    "missing_in_df = idx_df_paper - idx_df\n",
    "print(f\"Number of trials in df_paper not in df: {len(missing_in_df)}\")\n",
    "\n",
    "duplicates_df = df.index.duplicated().sum()\n",
    "print(\"Number of duplicate trial-ids in df:\", duplicates_df)\n",
    "\n",
    "duplicates_df_paper = df_paper.index.duplicated().sum()\n",
    "print(\"Number of duplicate trial-ids in df_paper:\", duplicates_df_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start      - 2025-04-07T20:26:38\n",
    "\n",
    "End        - 2025-04-08T18:53:41\n",
    "\n",
    "Time Delta - 80823 seconds (22.45 hours)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
